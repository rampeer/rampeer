Defining data pipelines

So, what do I want the language to be able to define? 
- Describe data schemas and validations in a language-agnostic way 
- Define and document micro-service components (nodes, services, pipelines, apps, systems and subsystems)
- Specify storages along with data retention policies, data indices, lags and data timing, data lineage,
and estimations of stored data size 
- Specify expected properties of requests (along with the intensity), SLAs of different parts of the system
- Simulate/estimate behaviour of the system under load
- Create _usable_ bindings for each language which transforms service description into code template (~DAO generation)
and tracks changes in the code to ensure that description matches the code.
- Create libraries of reusable patterns / "plug and play" services. Sorry, Docker, you still need configuration and 
figuring out how to interact with running service. 
- Generate SLA checkers, DAO and config boilerplates, deployment procedures, telemetry calls, and other useful things 
that can be code-generated from that description  

Why? Because I want ...

### Good data architecture design & documentation

Micro-service application consist of dozens of running applications, pipelines, tables and storages.
If you work in a large enough company, chances are that there is not _a single person_ that knows how 
every service and storage relates to each other, and which data is available where.

### Transform code into description and back

It is not uncommon to write validation logic several times (for example, client-side JS form validation + server
- side validation). It's fun, but I'd prefer generating code from the description, and define validation
logic once. We can make a step further. When code is changed, it is possible to update data schema.

### Actually usable documenting solution

Why no one uses UML systematically? 

Answer: it is too cumbersome and time-consuming. Also, most UML tools are not git-friendly (reviewing schema diffs is
impossible). Plant Text is popular for a reason.

So, I'd like to keep schema git-friendly and diff-able. Also, i'd like to check and update it to match the code.

### Less code, quicker prototyping

I complained about leaky interfaces. I'd like to enforce _easy_ interfaces, at least on the high level.

If you are installing OCR library, you can expect it to have a function that transforms image into text.
If you are installing a web crawler, you can expect it to have a function that gets data from the page.

It applies even more to the storages. If you are using ANY database, you can expect same - or similar interface.
Yes, essentially I want to hide the horror of NoSQL(Elastic / MongoDB) queries.  

To add extra spice, let's introduce shared libraries. They, along with generic interfaces and code generation,
provide quicker and less error-prone prototyping. Yes, these generic connectors won't be as fast as hand-crafted
requests/indices; and such interfaces hide fine-tuning configration of the databases.

But in most cases, you do not need that during prototyping.

### Deployment & debugging data 

When ins and outs of system are well-documented and defined, it is possible to intercept data.

Why? There are many cases:
- on exception, making a cross-microservice traceback along with a copy of all the data that was used to make response
- periodically make snapshots of database samples for development. If data connections are defined, we can
data samples which are small, yet matche production distributions and patterns. 
- intercept "live" data for monitoring and debugging. We may take out one component (or even function), edit it on
local machine, and redirect part of requests to it for testing.